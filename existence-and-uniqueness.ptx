<section xml:id="section-existence-and-uniqueness">
  <title>Existence and Uniqueness of Solutions</title>
  <introduction>
    <p>
      Before moving on with other techniques for solving first order
      equations, this is a nice place to take a pure-mathematical
      detour and talk about existence and uniqueness of solutions.
      This is very valuable material, even for the purposes of applied
      mathematics. To be able to analyze a differential equation and
      determine whether or not a solution should exist and whether it
      is unique is necessary for robust modelling with DEs. I'm going
      to deal only with first order equations where I can isolate the
      derivative term; that is, equations of the following form.
      <me>
        \frac{dy}{dx} = F(x,y)
      </me>
      The details of existence and uniqueness theorems rely on the
      properties of <m>F</m> as a function of two variables, which are
      very briefly summarized in <xref ref="section-functions" />. 
    </p>
  </introduction>
  <subsection xml:id="subsection-existence">
    <title>Existence</title>
    <p>
      Existence of solutions to first order DES is established by the
      Peano Existence Theorem.
    </p>
    <theorem>
      <statement>
        <p>
          In a differential equation of this form
          <me>
            \frac{dy}{dt} = F(y,t) 
          </me>, 
          if <m>F</m> is continuous in both variables in an open set
          <m>U \subset \RR^2</m> and if <m>(x_0,y_0) \in U</m>, then
          there exists <m>\epsilon > 0</m> such that the initial value
          problem associated to the DE and the initial condition
          <m>y(x_0) = y_0</m> has a solution with domain at least
          <m>[x_0-\epsilon,x_0+\epsilon]</m>.
        </p>
      </statement>
    </theorem>
    <p>
      This is a very local result: the small positive <m>\epsilon</m>
      only guarantees a tiny piece of a function as a solution.
      Existence (and later uniqueness) are only guaranteed very close
      to the initial value of the function. I do know that the
      function is differentiable in this small inteval, but I don't
      know anything else: outside the interval, anything could happen
      with the solution.
    </p>
    <p>
      It's also useful to note, particularly for students with
      experience in other senior mathematics classes, that this
      theorem relies on topological considerations: I need an
      <em>open</em> subset <m>U</m> where <m>F</m> is continuous. I'm
      not going to get into topology in this course; I'm not even
      going to define an open set in <m>\RR^2</m>. I just thouhgt I
      was worth pointing out the topological considerations for those
      who have seen these definitions elsewhere. 
    </p>
    <example>
      <statement>
        <p>
          <md>
            <mrow>
              \amp y^\prime = y^{\frac{2}{3}} \amp \amp y(0)=0
            </mrow>
          </md>
          This DE satisfies the conditions of the theorem, so a
          solutions exists. However, there are two solutions:
          <m>y=0</m> and <m>y=x^3/27</m>. Peano's theorem ensures that
          a solution exists, but doesn't imply the uniqueness of a
          solution.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          <md>
            <mrow>
              \amp y^\prime = \sqrt{|y|} \amp \amp y(0) = 0
            </mrow>
          </md>
          This <m>F</m> is also continuous near <m>(0,0)</m>, so a
          solution exists by Peano's theorm. This IVP is solved by
          <m>y=0</m> and <m>y = x^2/4</m>. It's obvious that
          something else is needed to ensure uniqueness.
        </p>
      </statement>
    </example>
    <p>
      Before I move on to the next result, I could wonder about the
      proof of the Peano Existence Theorem. Unforunately, that proof
      doesn't fall within the scope of this course. I would need to
      establish a number of new definitions and techniques from real
      analysis, as well as struggle through a bunch of tricky
      <m>\epsilon</m> and <m>\delta</m> arguments. It's interesting
      stuff, but unforutnately not part of this course.
    </p>
  </subsection>
  <subsection xml:id="subsection-lipschitz">
    <title>Lipschitz Continuity</title>
    <p>
      In order to state the theorem about uniqueness, I need a new
      definition of continuity. 
    </p>
    <definition>
      <statement>
        <p>
          A function from an open interval <m>U</m> in <m>\RR</m> to
          <m>\RR</m> is called <term>Lipschitz continuous</term> if
          <m>\exists K > 0</m> such that <m>\forall x_1,x_2 \in U</m>,
          it is true that <m>|f(x_1) - f(x_2)| \leq K|x_1-x_2|</m>.
        </p>
      </statement>
    </definition>
    <p>
      This is a strange kind of continuity. The definition is
      stronger than normal: Lipschitz continuity implies normal
      continuity. Moreover, the definition is global over <m>U</m>,
      not just local like conventional continuity. Therefore, the
      domain matters: a function might be Lipschitz continuous on a
      small interval, but not on a larger one.
    </p>
    <p>
      As an interpretation, Lipschitz continuity is a control on the
      global growth over the a specific interval. For a Lipschitz
      continuous function, there is a linear function that bounds the
      function on the designated interval. Unsurprisingly, this means that
      the defintion usually only works on bounded sets. For a more
      visual interpretation, the definition compares <m>f</m> to some
      other linear function <m>g(x) = K|x|</m>. The graph of
      <m>g</m> gives a cone in <m>\RR^2</m> and the <m>f</m> must stay
      inside this cone over an interval to be Lipschitz continuous.
      In this way, the definition limits the local growth of <m>f</m>
      and its derivatives.
    </p>
    <example>
      <statement>
        <p>
          Here are some examples to illustrate the idea of Lipschitz
          continuity.
        </p>
        <p><ul>
          <li>
            <m>f(x) = x</m> is Lipschitz continuous for <m>K=1</m> on
            any interval, since it is bounded by itself, a linear
            function.
          </li>
          <li>
            <m>f(x) = x^2</m> is Lipschitz continuous on any
            <em>bounded</em> interval. Specifically, on <m>(-7,7)</m>
            we can take <m>K=7</m>, since <m>-7x \leq x^2 \leq 7x</m>
            on this interval. However, <m>f(x) = x^2</m> is not
            Lipschitz continuous on all of <m>\RR</m>, since no linear
            function bounds it.
          </li>
          <li>
            <m>f(x) = x^{\frac{2}{3}}</m> is not Lipschitz continuous
            on <m>(-1,1)</m>, since its slope gets arbitrarily steep
            near the origin. This means that very close to <m>0</m>,
            it cannot be bounded by any linear function through
            <m>(0,0)</m>.
          </li>
        </ul></p>
      </statement>
    </example>
    <p>
      The example <m>f(x) = x^{\frac{2}{3}}</m> was not Lipschitz
      continuous at <m>0</m>, and it also failed to be differentiable
      at that point. I might wonder if differentiability is a
      sufficient condition. That would be convenient, since I know
      how to check for differentiability. However, consider a strange
      example.
    </p>
    <example>
      <statement>
        <p>
          <me>
            f(X) = \left\{ \begin{matrix} 
              x^{\frac{3}{2}} \sin \left( \frac{1}{x} \right) \amp x \in
              (0,1] \\ 
              0 \amp x = 0
            \end{matrix} \right.
          </me>
          This isn't Lipschitz continuous, but it is differentiable
          near zero, showing that differentiability isn't sufficient.
          However, there is some good news: this example is a strange
          aberation.
        </p>
      </statement>
    </example>
    <proposition>
      <statement>
        <p>
          A function which is <m>C^1</m> at a point <m>a</m> in its
          domain is also Lipschitz continuous on a small interval
          containing <m>a</m>. 
        </p>
      </statement>
    </proposition>
    <p>
      <m>f \in C^1</m> is roughly equivalent to saying that
      <m>\frac{\del f}{\del y}</m> must exist and be bounded.  This
      last criterion is the one I will use in practice: To check
      Lipschitz continuity, I can check if the derivatives exists and
      if it is bounded.
    </p>
    <p>
      Consider the <m>f(x) = x^2</m>. The derivaitve is <m>2x</m>,
      which is bounded locally near <m>0</m>. <m>f(x) =
      x^\frac{2}{3}</m> has derivative <m>\frac{2}{3}
      x^{\frac{-1}{3}}</m> which is unbounded near 0. The former
      function is Lipschitz continuous on finite intervals centred at
      the origin and the later is not.
    </p>
  </subsection>
  <subsection xml:id="subsection-uniqueness">
    <title>Uniqueness</title>
    <p>
      The theorem for uniqueness is called the Picard-Lindelh√∂f
      thereom. It is a small improvement and adjustment of the Peano
      existence theorem.
    </p>
    <theorem>
      <statement>
        <p>
          If <m>F</m> is Lipschitz continuous in y and continuous (in
          the ordinary sense) in x on an open set <m>U</m> in
          <m>\RR^2</m> and if <m>(x_0,y_0) \in U</m> then the initial
          value problem
          <md>
            <mrow>
              \amp \frac{dy}{dx} = F(x,y) \amp \amp y(x_0) = y_0
            </mrow>
          </md>
          has a unique solution <m>y=f(x)</m> defined on the domain
          <m>[x_0-\epsilon,x_0+\epsilon]</m> for some <m>\epsilon >
          0</m>.
        </p>
      </statement>
    </theorem>
    <p>
      All of the comments from the previous theorem apply here. The
      result is very local, relies on topology, and the proof is
      beyond the scope of the course. Before moving to examples, let
      me make an important point about the direction of implication in
      this theorem. The theorem says that the conditions (continuity
      in <m>x</m> and Lipschitz continuity in <m>y</m>) guarantee a
      unique solution. This is only a one-directional implication. A
      DE with a unique solution does not necessarily have to be
      Lipschitz continuous in <m>y</m>. In particular, this cannot be
      used to prove multiple solutions by showing the failure of
      Lipshchitz continuity, since the theorem doesn't include that
      second implication. 
    </p>
    <example>
      <statement>
        <p>
          <me>
            \frac{dy}{dx} = x \sqrt{y}
          </me>
          Let me considered the same example as before. This DE had
          multiple solutions for initial value <m>y(0) = 0</m>. The
          function <m>F(x,y) = x\sqrt{y}</m> has <m>y</m> derivative
          <m>\frac{\del F}{\del y} = \frac{x}{2\sqrt{y}}</m>, which is
          unbounded near <m>0</m>. Therefore, it is not Lipschitz
          continuous and doesn't satisfy the continious of the
          Picard-Lidenlh√∂f theorem, meaning that I can't be assured of
          a unique solution. (Again, the failure of Lipschitz
          continuity does guarantee multiple solutions, just allows
          for the possibility.) 
        </p>
      </statement>
    </example>
    <p>
      Lastly, anticipating the equations in <xref
      ref="section-linear-des" />, consider the general first-order
      linear DE where <m>P</m> and <m>Q</m> are continuous functions.
      <me>
        \frac{dy}{dx} = Q(x) - P(x) y
      </me>
      In this case, <m>F(x,y) = Q(x) - P(x)y</m> and the <m>y</m>
      derivative is <m>-P(x)</m>, which is bounded assuming that
      <m>P</m> is continuous. I can apply the Picard-Lindelh√∂f
      theorem to conclude that linear equations have unique solutions
      whenever their coefficient functions <m>P</m> and <m>Q</m> are
      continuous. I can go into the next section of the course with
      assurance that unique solutions will exist, as long as <m>P</m>
      is continuous. 
    </p>
  </subsection>
</section>
