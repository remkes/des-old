<section xml:id="regular-singular-points">
  <title>Regular Singular Points</title>
  <introduction>
    <p>
      Throughout this section,
      we are considering this second order linear homogeneous differential equation.
      <me>
        y^{\prime \prime} + P(t) y^\prime + Q(t) y = 0
      </me>
    </p>
    <p>
      Recall that this differential equation has a regular point if <m>P(t)</m> and <m>Q(t)</m> are analytic at that point.
      We've dealt with solutions at ordinary points;
      now we will consider solutions at singular points.
      There are particular types of singular points which are reasonable to deal with,
      since the functions <m>P</m> and <m>Q</m> are quite close to being analytic.
      Here is the definition:
    </p>
    <definition>
      <statement>
        <p>
          A singular point <m>t_0</m> of this DE is called a
          <term>regular singular point</term>
          if the two functions <m>(t-t_0)P(t)</m> and
          <m>(t-t_0)^2 Q(t)</m> are analytic at <m>t-0</m>.
          Equivalently,
          <m>t_0</m> is a regular singular point if the limits of
          <m>(t-t_0) P(t)</m> and <m>(t-t_0)^2 Q(t)</m> as
          <m>t \rightarrow t_0</m> exist and are finite. (In complex variables terminology,
          <m>P(t)</m> and <m>Q(t)</m> have to have poles,
          not essential singaularities,
          the pole of <m>P</m> has to be order 1, and the pole of <m>Q</m> has to be order 1 or 2.) Note that <m>P</m> and <m>Q</m> don't have to be rational functions for this definition.
        </p>
      </statement>
    </definition>
  </introduction>
  <subsection xml:id="examples-rsp">
    <title>Examples of Regular Singular Points</title>
    <example>
      <statement>
        <me>
          y^{\prime \prime} + \frac{y^\prime}{t^3(t-1)^2(t-2)(t-3)(t-4)} +
          \frac{y}{t^2 (t-1)(t-2)^2 (t-3)^2 (t-4)^3} = 0
        </me>
        <p>
          All points other than <m>t=0,1,2,3,4</m> are ordinary points.
          Of the singular points, only <m>t=2,3</m> are regular singular points.
          The singular points <m>t=0,1</m> have <m>P(t)</m> with a pole of order 2 or higher (a square term or worse in the denominator),
          which is not allowed.
          The singular point <m>t=4</m> has <m>Q(t)</m> with a pole of order 3 (a cubic term in the denominator),
          which is also not allowed.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <me>
          y^{\prime \prime} + \cot t y^\prime + y = 0
        </me>
        <p>
          We use a limit to find the singular points.
          <me>
            \lim_{t \rightarrow 0} t \cot t = \lim_{t \rightarrow 0} \frac{ t \cos t}{\sin t} = 1
          </me>
        </p>
        <p>
          The limit shows that <m>t \cot t</m> is analytic at <m>t=0</m>,
          so <m>t=0</m> is a regular singular point.
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="frobenius">
    <title>The Method of Frobenius</title>
    <p>
      The method of Frobenius is a method of constructing solutions at regular singular points.
      It relies on this existence theorem.
    </p>
    <theorem>
      <statement>
        <p>
          Consider a second order homogeneous linear differential equation.
          <me>
            y^{\prime \prime} + P(t) y^\prime + Q(t) = 0
          </me>.
        </p>
        <p>
          If <m>t_0</m> is a regular singular point of this equation,
          then there exists at least one
          (but possibly two)
          <m>r \in \RR</m> such that we have a series solution of this form (with <m>c_0 \neq 0)</m>).
          <me>
            y = (t-t_0)^r \sum_{n=0}^\infty c_n (t-t_0)^n
          </me>
        </p>
      </statement>
    </theorem>
    <p>
      This is an extension of the idea of analytic solutions.
      This solution is analytic if <m>r \in \NN</m>.
      Otherwise, it is very close, only differing by this multiple <m>(t-t_0)^r</m>.
      If <m>r</m> is any negative real number,
      we have an asymptote at <m>t=t_0</m>,
      so we can't evaluate there.
      However, we expect convergence on <m>0 \lt |t-t_0| \lt R</m>.
      If <m>r</m> is a negative integer,
      we have a new name for these series.
    </p>
    <definition>
      <statement>
        <p>
          A <term>Laurent series</term> is a series of the form
          <me>
            \sum_{n=-\infty}^\infty c_n (t-t_0)^n
          </me>
          where <m>c_n \in \RR</m>.
          This is exactly the form of a Taylor series,
          but we now allow negative exponents.
        </p>
      </statement>
    </definition>
    <p>
      Now that we have this theorem,
      and the form of the solution, we proceed as usual:
      we throw the form into the differential equation and see what happens.
      We expect to have a similar process for finding the coefficient <m>c_n</m> of the series.
      However, we also need a process for finding the number <m>r</m>.
      Let's assume, for convenience in this derivation,
      that <m>t_0 = 0</m> is the regular singular point.
    </p>
    <p>
      We know that <m>tP(t)</m> and <m>t^2 Q(t)</m> are analytic,
      from the definition of a regular singular point.
      This means that <m>P(t)</m> and <m>Q(t)</m> can be written in the form
      <me>
        P(t) = \frac{p_{-1}}{t} + \sum_{n=0}^\infty p_n t^n \hspace{2cm} Q(t) = \frac{q_{-2}}{t^2} + \frac{q_{-1}}{t} + \sum_{n=0}^\infty q_n t^n
      </me>.
    </p>
    <p>
      We will call these Laurent forms for <m>P</m> and <m>Q</m>.
      The coefficients <m>p_1</m> and <m>q_2</m> will be useful a bit later,
      so here are two useful identities to calculate <m>p_1</m> and <m>q_2</m>.
      <me>
        p_{-1} = \lim_{t \rightarrow 0 } t P(t) \hspace{2cm} q_{-2} = \lim_{t \rightarrow 0 } t^2 Q(t) \hspace{2cm}
      </me>
    </p>
    <p>
      Now we start the method of Frobenious by calculating the derivatives of <m>y</m> in this new series form.
      <md>
        <mrow>y \amp  = t^r \sum_{n=0}^\infty c_n t^n = \sum_{n=0}^\infty c_n t^{n+r}</mrow>
        <mrow>y^\prime \amp  = \sum_{n=0}^\infty c_n (n+r) t^{n+r-1}</mrow>
        <mrow>y^{\prime \prime} \amp  = \sum_{n=0}^\infty c_n (n+r) (n+r-1) t^{n+r-2}</mrow>
      </md>
    </p>
    <p>
      (Notice that we don't necessarily lose terms when taking derivatives;
      if <m>r</m> is not an integer,
      there are no constant terms in the series which go to zero under differentiation.
      If <m>r</m> is an integer,
      we should make a note to worry about derivatives setting constant terms to zero.)
    </p>
    <p>
      With these expressions for <m>y</m>,
      <m>P</m> and <m>Q</m>, we put it all together into the original DE.
      <md>
        <mrow>\sum_{n=0}^\infty c_n (n+r) (n+r-1) t^{n+r-2} + P(t) \sum_{n=0}^\infty c_n (n+r) t^{n+r-1} + Q(t) \sum_{n=0}^\infty c_n t^{n+r} \amp  = 0</mrow>
        <mrow>\sum_{n=0}^\infty c_n (n+r) (n+r-1) t^{n+r-2} + \left(\frac{p_{-1}}{t} + \sum_{n=0}^\infty p_n t^n \right) \sum_{n=0}^\infty c_n (n+r) t^{n+r-1} \amp</mrow>
        <mrow>+ \left( \frac{q_{-2}}{t^2} + \frac{q_{-1}}{t} + \sum_{n=0}^\infty q_n t^n \right) \sum_{n=0}^\infty c_n t^{n+r} \amp  = 0</mrow>
      </md>
    </p>
    <p>
      This is quite a mess:
      we have <m>r</m> to determine as well as the series coefficients.
      However, we can focus on the coefficient of the leading term
      (<m>t^{r-2}</m>).
      <md>
        <mrow>r(r-1) c_0 t^{r-2} + \frac{p_{-1}}{t} r c_0 t^{r-1} + \frac{q_{-2}}{t^2} c_0 t^r \amp  = 0</mrow>
        <mrow>(r(r-1) + p_{-1} r + q_{-2}) c_0 t^{r-2} \amp  = 0</mrow>
        <mrow>r(r-1) + p_{-1}r + q_{-2} \amp  = 0 \hspace{1cm} \text{ using the fact that }  c_0 \neq 0</mrow>
      </md>
    </p>
    <definition>
      <statement>
        <p>
          This quadratic determines <m>r</m>.
          It is called the <term>indicial equation</term>.
        </p>
      </statement>
    </definition>
    <p>
      So, before we proceed to find the recurrence relations and the series coefficients,
      we use this equation to determine <m>r</m>.
      After finding <m>r</m>,
      the method looks very similar to solutions at ordinary points.
      If there are two real roots of the indicial equation,
      is seems we'll have to repeat the same process for each root.
      However, in practice,
      we can leave <m>r</m> undetermined for most of the procees.
    </p>
  </subsection>
  <subsection xml:id="frobenius-examples">
    <title>Examples</title>
    <example>
      <statement>
        <md>
          <mrow>3ty^{\prime \prime} + y^\prime - y \amp  = 0</mrow>
          <mrow>P \amp  = \frac{1}{3t}</mrow>
          <mrow>Q \amp  = \frac{-1}{3t}</mrow>
          <mrow>tP \amp  = \frac{1}{3} \implies p_{-1} = \frac{1}{3}</mrow>
          <mrow>t^2 Q \amp  = \frac{-t}{3} \implies q_{-2} = 0</mrow>
          <mrow>r(r-1) + \frac{r}{3} \amp  = 0</mrow>
          <mrow>3r^2 - 2r = 0 \implies r \amp  = 0 \text{ or }  r = \frac{2}{3}</mrow>
        </md>
        <p>
          We'll deal with the <m>r=0</m> case first.
          (Note that <m>r=0</m> means we get a conventional Taylor series solution.)
          <md>
            <mrow>3t \sum_{n=2}^\infty c_n (n) (n-1) t^{n-2} + \sum_{n=1}^\infty c_n (n) t^{n-1} - \sum_{n=0}^\infty c_n t^{n} \amp  = 0</mrow>
            <mrow>\sum_{n=2}^\infty 3c_n (n) (n-1) t^{n-1} + \sum_{n=1}^\infty c_n (n) t^{n-1} - \sum_{n=0}^\infty c_n t^{n} \amp  = 0</mrow>
            <mrow>\sum_{n=1}^\infty 3c_{n+1} (n+1) n t^n + \sum_{n=1}^\infty c_{n+1} (n+1) t^n - \sum_{n=0}^\infty c_n t^{n} \amp  = 0</mrow>
            <mrow>c_1 - c_0 + \sum_{n=1}^\infty \left[ 3(n+1)n c_{n+1} + (n+1) c_{n+1} - c_n \right] t^n \amp  = 0</mrow>
            <mrow>c_{n+1} = \frac{c_n}{3(n^2+n) + n+1} = \frac{c_n}{3n^2 +4n +1} \amp  = \frac{c_n}{(3n+1)(n+1)}</mrow>
          </md>
        </p>
        <p>
          We have the recurrence relation
          (which is first order this time),
          so we start calculating terms.
          <md>
            <mrow>c_0 \amp  = c_0</mrow>
            <mrow>c_1 \amp  = c_0</mrow>
            <mrow>c_2 \amp  = \frac{c_1}{(4)(2)} = \frac{c_0}{(4)(2)}</mrow>
            <mrow>c_3 \amp  = \frac{c_2}{(7)(3)} = \frac{c_0}{(7)(4)(3)(2)}</mrow>
            <mrow>c_4 \amp  = \frac{c_3}{(10)(4)} = \frac{c_0}{(10)(7)(4)(4)(3)(2)}</mrow>
            <mrow>c_5 \amp  = \frac{c_4}{(13)(5)} = \frac{c_0}{(13)(10)(7)(4) (5)(4)(3)(2)}</mrow>
            <mrow>c_n \amp  = \frac{c_0}{n! (4)(7)(10) \ldots (3n-2)}</mrow>
            <mrow>y_1 \amp  = 1 + \sum_{n=1}^\infty \frac{t^n}{n! (4)(7)(10) \ldots (3n-2)}</mrow>
          </md>
        </p>
        <p>
          This is the solution for <m>r=0</m>.
          Now we proceed to the <m>r=\frac{2}{3}</m> case.
          (This is a non-integer exponent,
          so the solution is not a conventional Taylor series.)
          <md>
            <mrow>3t \sum_{n=0}^\infty c_n \left(n + \frac{2}{3} \right) \left(n + \frac{2}{3}-1 \right) t^{n + \frac{2}{3}-2} + \sum_{n=0}^\infty c_n \left(n + \frac{2}{3} \right) t^{n + \frac{2}{3}-1} - \sum_{n=0}^\infty c_n t^{n + \frac{2}{3}} \amp  = 0</mrow>
            <mrow>t^{\frac{2}{3}} \left[ \sum_{n=0}^\infty 3c_n \left(n+ \frac{2}{3} \right) \left(n - \frac{1}{3} \right) t^{n-1} + \sum_{n=0}^\infty c_n \left(n+ \frac{2}{3} \right) t^{n-1} - \sum_{n=0}^\infty c_n t^{n} \right] \amp  = 0</mrow>
            <mrow>t^{\frac{2}{3}} \left[ \sum_{n=0}^\infty 3c_n \left(n + \frac{2}{3} \right) \left( n - \frac{1}{3} \right) t^{n -1} + \sum_{n=0}^\infty c_n \left(n + \frac{2}{3} \right) t^{n-1} - \sum_{n=1}^\infty c_{n-1} t^{n-1} \right] \amp  = 0</mrow>
            <mrow>t^{\frac{2}{3}} \left[ \left(3\frac{2}{3} \frac{-1}{3} + \frac{2}{3} \right) c_0 t^{-1} + \sum_{n=1}^\infty \left[ 3 c_n \left( n+ \frac{2}{3} \right) \left( n - \frac{1}{3} \right) + c_n \left( n + \frac{2}{3} \right) - c_{n-1} \right] t^{n-1} \right] \amp  = 0</mrow>
            <mrow>\left[ \left( n + \frac{2}{3} \right) \left( 3n - 1 + 1 \right) \right] c_n - c_{n-1} \amp  = 0</mrow>
            <mrow>c_n = \frac{c_{n-1}}{ \left( n + \frac{2}{3} \right) (3n)} = \frac{c_{n-1}}{(3n+2)n} \amp</mrow>
            <mrow>c_{n+1} = \frac{c_n}{(3n+5)(n+1)} \amp</mrow>
          </md>
        </p>
        <p>
          The term in front of <m>c_0</m> reduces to <m>0</m>, so <m>c_0</m> is free.
          After the first term, we have a recurrence relation,
          so we start calculating terms.
          <md>
            <mrow>c_0 \amp  = c_0</mrow>
            <mrow>c_1 \amp  = \frac{c_0}{5}</mrow>
            <mrow>c_2 \amp  = \frac{c_1}{(8)(2)} = \frac{c_0}{(8)(5)(2)}</mrow>
            <mrow>c_3 \amp  = \frac{c_2}{(11)(3)} = \frac{c_0}{(5)(8)(11)(2)(3)}</mrow>
            <mrow>c_4 \amp  = \frac{c_3}{(14)(4)} = \frac{c_0}{(5)(8)(11)(14)(2)(3)(4)}</mrow>
            <mrow>c_5 \amp  = \frac{c_4}{(17)(5)} = \frac{c_0}{(5)(8)(11)(14)(17)(2)(3)(4)(5)}</mrow>
            <mrow>c_n \amp  = \frac{c_0}{n! (5)(8)(11) \ldots (3n+2)}</mrow>
            <mrow>y_2 \amp  = 1 + \sum_{n=1}^\infty \frac{t^{n + \frac{2}{3}}}{n! (5)(8) \ldots (3n+2)}</mrow>
            <mrow>y \amp  = A \left( 1 + \sum_{n=1}^\infty \frac{t^n}{n! (4)(7)(10) \ldots (3n-2)} \right) + B \left( 1 + \sum_{n=1}^\infty \frac{t^{n + \frac{2}{3}}}{n! (5)(8) \ldots (3n+2)} \right)</mrow>
          </md>
        </p>
        <p>
          The radius of convergence here is <m>R = \infty</m>,
          which we can find by ratio test.
          It is good to note, though,
          that there are no guarantees about the radius of convergence with the method of Frobenius.
        </p>
        <p>
          In this example,
          we could have gone as far as the recurrence relation using an arbitrary <m>r</m> to avoid repetition.
          We do need to specify <m>r</m> once we get to the recurrence relation.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <me>
          ty^{\prime \prime} + y = 0
        </me>
        <p>
          Here <m>P =0</m> and <m>Q=\frac{1}{t}</m>,
          so <m>p_{-1} = 0</m> and <m>q_{-2} = 0</m>.
          That means the indicial equation is
          <m>r(r-1) = 0</m> with roots <m>r=0</m> and <m>r=1</m>.
          We start with the case <m>r=0</m>.
          <md>
            <mrow>t \sum_{n=2}^\infty n(n-1)c_n t^{n-2} + \sum_{n=0}^\infty c_n t^n \amp  = 0</mrow>
            <mrow>\sum_{n=2}^\infty n(n-1)c_n t^{n-1} + \sum_{n=0}^\infty c_n t^n \amp  = 0</mrow>
            <mrow>\sum_{n=1}^\infty (n+1)nc_n t^n + \sum_{n=0}^\infty c_n t^n \amp  = 0</mrow>
            <mrow>c_0 + \sum_{n=1}^\infty \left[ (n+1)nc_n + c_n \right] t^n \amp  = 0</mrow>
            <mrow>c_{n+1} \amp  = \frac{-c_n}{(n+1)(n)}</mrow>
            <mrow>c_0 \amp  = 0</mrow>
            <mrow>c_1 \amp  = c_1</mrow>
            <mrow>c_2 \amp  = \frac{-c_1}{2}</mrow>
            <mrow>c_3 \amp  = \frac{-c_2}{(3)(2)} = \frac{c_1}{(3)(2)}</mrow>
            <mrow>c_4 \amp  = \frac{-c_3}{(4)(3)} = \frac{-c_1}{(4)(3)(3)(2)(2)}</mrow>
            <mrow>c_5 \amp  = \frac{-c_4}{(5)(4)} = \frac{c_1}{(5)(4)(4)(3)(3)(2)(2)}</mrow>
            <mrow>c_n \amp  = \frac{(-1)^{n+1} c_1}{n! (n-1)!}</mrow>
            <mrow>y_1 \amp  = \sum_{n=1}^\infty \frac{(-1)^{n+1} t^n}{n!(n-1)!}</mrow>
          </md>
        </p>
        <p>
          Then we calculate with <m>r=1</m>.
          Note the bounds in this case:
          since the series doesn't have a constant term,
          we only loose one term in the second derivative.
          <md>
            <mrow>t \sum_{n=1}^\infty (n+1)nc_n t^{n-1} + \sum_{n=0}^\infty c_n t^{n+1} \amp  = 0</mrow>
            <mrow>\sum_{n=1}^\infty (n+1)nc_n t^n + \sum_{n=0}^\infty c_n t^{n+1} \amp  = 0</mrow>
            <mrow>\sum_{n=1}^\infty (n+1)nc_n t^n + \sum_{n=1}^\infty c_{n-1} t^n+ \amp  = 0</mrow>
            <mrow>(n+1)n c_n + c_{n-1} \amp  = 0</mrow>
            <mrow>c_n \amp  = \frac{-c_{n-1}}{(n+1)(n)}</mrow>
            <mrow>c_{n+1} \amp  = \frac{-c_n}{(n+2)(n+1)}</mrow>
            <mrow>c_0 \amp  = c_0</mrow>
            <mrow>c_1 \amp  = \frac{-c_0}{2}</mrow>
            <mrow>c_2 \amp  = \frac{-c_1}{(3)(2)} = \frac{c_0}{(3)(2)}</mrow>
            <mrow>c_3 \amp  = \frac{-c_2}{(4)(3)} = \frac{-c_0}{(4)(3)(3)(2)(2)}</mrow>
            <mrow>c_4 \amp  = \frac{-c_3}{(5)(4)} = \frac{c_0}{(5)(4)(4)(3)(3)(2)(2)}</mrow>
            <mrow>c_n \amp  = \frac{(-1)^{n} c_0}{(n+1)! n!}</mrow>
            <mrow>y_1 \amp  = \sum_{n=0}^\infty \frac{(-1)^n t^{n+1}}{(n+1)!n!}</mrow>
            <mrow>y_1 \amp  = \sum_{n=1}^\infty \frac{(-1)^{n+1} t^n}{n!(n-1)!}</mrow>
          </md>
        </p>
        <p>
          Very curiously, we get the same series.
          The two roots don't lead to two independent series, but to the same series.
          We would need other information to get the second solutions.
          In general, finding another solution can be quite difficult.
          With the method of Frobenius,
          we are not guaranteed to find both solutions.
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="frobenius-multiple-solutions">
    <title>Multiple Solutions in the Method of Frobenius</title>
    <p>
      We have a theorem which deals with the situation in the previous example,
      where both roots gave the same series.
    </p>
    <theorem>
      <statement>
        <p>
          In the setting of the method of Frobenius,
          assume that <m>r_1</m> and <m>r_2</m> are two real roots of the indicial equation.
          Without loss of generalization,
          assume that <m>r_1 \geq r_2</m>.
          Then there are three cases.
          <ul>
            <li>
              <title>Case 1:</title>
              <p>
                If <m>r_1 - r_2 \notin \NN</m> then each root will derive a linearly independent series solutions.
                (The idea here is that different fractional or irrational exponents lead to radically different behaviour.)
              </p>
            </li>
            <li>
              <title>Case 2:</title>
              <p>
                If <m>r_1 - r_2 \in \NN</m> but <m>r_1 \neq r_2</m>,
                then the second root may not produce a new linearly independent solution.
                If it doesn't, and both roots produce the solution <m>y_1</m>,
                then a second solution exists and has the form:
                <me>
                  y_2 = a y_1 \ln t + \sum_{n=0}^\infty b_n t^{n+r_2}
                </me>.
                In this case,
                we would need to do extra work to find the constant <m>a</m> and the series coefficients <m>b_n</m>.
              </p>
            </li>
            <li>
              <title>Case 3</title>
              <p>
                If the two roots are the same,
                then (obviously) only one series solution is generated.
                A second solution exists and has the same form as the previous case.
              </p>
            </li>
          </ul>
        </p>
      </statement>
    </theorem>
    <p>
      The proof of this theorem relies on another differential equation technique called
      <em>reduction of order</em>.
      I've chosen not to cover that technique in this courses,
      but it is good to be aware that it exists.
      In general, if we have one solution to a second order linear DE, then reduction of order is a method for using that solution to change the second order DE into a first order DE, which can then be solved by first order methods.
    </p>
    <theorem>
      <title>Reduction of Order</title>
      <statement>
        <p>
          Assume that <m>y_1</m> is a solution of homogeneous linear second order DE of the form
          <me>
            y^{\prime \prime} + P(t) y^\prime + Q(t) = 0
          </me>.
        </p>
        <p>
          Then a second linearly independent is given by the formula
          <me>
            y_2 = y_1(t) \int \frac{e^{- \int P(t) dt}}{y_1^2(t)} dt
          </me>.
        </p>
        <p>
          This reduction of order technique is fairly impractical for series solution,
          since dividing by <m>y_1^2</m> and then integrating is a miserable calculation.
        </p>
      </statement>
    </theorem>
  </subsection>
</section>