<section xml:id="section-regular-singular-points">
  <title>Regular Singular Points</title>
  <introduction>
    <p>
      Throughout this section, we are considering this second order
      linear homogeneous differential equation.
      <me>
        y^{\prime \prime} + P(t) y^\prime + Q(t) y = 0
      </me>
      Recall that this differential equation has a regular point if
      <m>P(t)</m> and <m>Q(t)</m> are analytic at that point. We've
      dealt with solutions at ordinary points; now we will consider
      solutions at singular points. There are particular types of
      singular points which are reasonable to deal with, since the
      functions <m>P</m> and <m>Q</m> are quite close to being
      analytic. Here is the definition:
    </p>
    <definition>
      <statement>
        <p>
          A singular point <m>t_0</m> of this DE is called a
          <term>regular singular point</term> if the two functions
          <m>(t-t_0)P(t)</m> and <m>(t-t_0)^2 Q(t)</m> are analytic at
          <m>t-0</m>. Equivalently, <m>t_0</m> is a regular singular
          point if the limits of <m>(t-t_0) P(t)</m> and <m>(t-t_0)^2
          Q(t)</m> as <m>t \rightarrow t_0</m> exist and are finite.
          (In complex variables terminology, <m>P(t)</m> and
          <m>Q(t)</m> have to have poles, not essential
          singaularities, the pole of <m>P</m> has to be order 1, and
          the pole of <m>Q</m> has to be order 1 or 2.) Note that
          <m>P</m> and <m>Q</m> don't have to be rational functions
          for this definition.
        </p>
      </statement>
    </definition>
  </introduction>
  <subsection xml:id="subsection-examples-rsp">
    <title>Examples of Regular Singular Points</title>
    <example>
      <statement>
        <p>
          <me>
            y^{\prime \prime} +
            \frac{y^\prime}{t^3(t-1)^2(t-2)(t-3)(t-4)} + \frac{y}{t^2
            (t-1)(t-2)^2 (t-3)^2 (t-4)^3} = 0
          </me>
          All points other than <m>t=0,1,2,3,4</m> are ordinary
          points. Of the singular points, only <m>t=2,3</m> are
          regular singular points. The singular points <m>t=0,1</m>
          have <m>P(t)</m> with a pole of order 2 or higher (a square
          term or worse in the denominator), which is not allowed.
          The singular point <m>t=4</m> has <m>Q(t)</m> with a pole of
          order 3 (a cubic term in the denominator), which is also not
          allowed.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          <me>
            y^{\prime \prime} + \cot t y^\prime + y = 0
          </me>
          We use a limit to find the singular points.
          <me>
            \lim_{t \rightarrow 0} t \cot t = \lim_{t \rightarrow 0}
            \frac{ t \cos t}{\sin t} = 1
          </me>
          The limit shows that <m>t \cot t</m> is analytic at
          <m>t=0</m>, so <m>t=0</m> is a regular singular point.
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="subsection-frobenius">
    <title>The Method of Frobenius</title>
    <p>
      The method of Frobenius is a method of constructing solutions at
      regular singular points. It relies on this existence theorem.
    </p>
    <theorem>
      <statement>
        <p>
          Consider a second order homogeneous linear differential
          equation.
          <me>
            y^{\prime \prime} + P(t) y^\prime + Q(t) = 0
          </me>.
          If <m>t_0</m> is a regular singular point of this equation,
          then there exists at least one (but possibly two) <m>r \in
          \RR</m> such that we have a series solution of this form
          (with <m>c_0 \neq 0)</m>).
          <me>
            y = (t-t_0)^r \sum_{n=0}^\infty c_n (t-t_0)^n
          </me>
        </p>
      </statement>
    </theorem>
    <p>
      This is an extension of the idea of analytic solutions. This
      solution is analytic if <m>r \in \NN</m>. Otherwise, it is very
      close, only differing by this multiple <m>(t-t_0)^r</m>. If
      <m>r</m> is any negative real number, we have an asymptote at
      <m>t=t_0</m>, so we can't evaluate there. However, we expect
      convergence on <m>0 \lt |t-t_0| \lt R</m>. If <m>r</m> is a
      negative integer, we have a new name for these series.
    </p>
    <definition>
      <statement>
        <p>
          A <term>Laurent series</term> is a series of the form
          <me>
            \sum_{n=-\infty}^\infty c_n (t-t_0)^n
          </me>
          where <m>c_n \in \RR</m>. This is exactly the form of a
          Taylor series, but we now allow negative exponents.
        </p>
      </statement>
    </definition>
    <p>
      Now that we have this theorem, and the form of the solution, we
      proceed as usual: we throw the form into the differential
      equation and see what happens. We expect to have a similar
      process for finding the coefficient <m>c_n</m> of the series.
      However, we also need a process for finding the number <m>r</m>.
      Let's assume, for convenience in this derivation, that <m>t_0 =
      0</m> is the regular singular point.
    </p>
    <p>
      We know that <m>tP(t)</m> and <m>t^2 Q(t)</m> are analytic, from
      the definition of a regular singular point. This means that
      <m>P(t)</m> and <m>Q(t)</m> can be written in the form
      <md>
        <mrow>
          \amp P(t) = \frac{p_{-1}}{t} + \sum_{n=0}^\infty p_n t^n
          \amp \amp Q(t) = \frac{q_{-2}}{t^2} + \frac{q_{-1}}{t} +
          \sum_{n=0}^\infty q_n t^n
        </mrow>
      </md>.
      We will call these Laurent forms for <m>P</m> and <m>Q</m>. The
      coefficients <m>p_1</m> and <m>q_2</m> will be useful a bit
      later, so here are two useful identities to calculate <m>p_1</m>
      and <m>q_2</m>.
      <md>
        <mrow>
          \amp p_{-1} = \lim_{t \rightarrow 0 } t P(t) \amp \amp
          q_{-2} = \lim_{t \rightarrow 0 } t^2 Q(t) 
        </mrow>
      </md>
      Now we start the method of Frobenious by calculating the
      derivatives of <m>y</m> in this new series form. 
      <md>
        <mrow>
          y \amp = t^r \sum_{n=0}^\infty c_n t^n = \sum_{n=0}^\infty
          c_n t^{n+r}
        </mrow>
        <mrow>
          y^\prime \amp = \sum_{n=0}^\infty c_n (n+r) t^{n+r-1}
        </mrow>
        <mrow>
          y^{\prime \prime} \amp = \sum_{n=0}^\infty c_n (n+r) (n+r-1)
          t^{n+r-2}
        </mrow>
      </md>
      (Notice that we don't necessarily lose terms when taking
      derivatives; if <m>r</m> is not an integer, there are no
      constant terms in the series which go to zero under
      differentiation. If <m>r</m> is an integer, we should make a
      note to worry about derivatives setting constant terms to
      zero.)
    </p>
    <p>
      With these expressions for <m>y</m>, <m>P</m> and <m>Q</m>, we
      put it all together into the original DE.
      <md>
        <mrow>
          \sum_{n=0}^\infty c_n (n+r) (n+r-1) t^{n+r-2} + P(t)
          \sum_{n=0}^\infty c_n (n+r) t^{n+r-1} + Q(t)
          \sum_{n=0}^\infty c_n t^{n+r} \amp = 0
        </mrow>
        <mrow>
          \sum_{n=0}^\infty c_n (n+r) (n+r-1) t^{n+r-2} +
          \left(\frac{p_{-1}}{t} + \sum_{n=0}^\infty p_n t^n \right)
          \sum_{n=0}^\infty c_n (n+r) t^{n+r-1} \amp
        </mrow>
        <mrow>
          + \left( \frac{q_{-2}}{t^2} + \frac{q_{-1}}{t} +
          \sum_{n=0}^\infty q_n t^n \right) \sum_{n=0}^\infty c_n
          t^{n+r} \amp = 0
        </mrow>
      </md>
      This is quite a mess: we have <m>r</m> to determine as well as
      the series coefficients. However, we can focus on the
      coefficient of the leading term (<m>t^{r-2}</m>).
      <md>
        <mrow>
          r(r-1) c_0 t^{r-2} + \frac{p_{-1}}{t} r c_0 t^{r-1} +
          \frac{q_{-2}}{t^2} c_0 t^r \amp = 0
        </mrow>
        <mrow>
          (r(r-1) + p_{-1} r + q_{-2}) c_0 t^{r-2} \amp = 0
        </mrow>
        <mrow>
          r(r-1) + p_{-1}r + q_{-2} \amp = 0 \text{ using
          the fact that }  c_0 \neq 0
        </mrow>
      </md>
    </p>
    <definition>
      <statement>
        <p>
          This quadratic determines <m>r</m>. It is called the
          <term>indicial equation</term>.
        </p>
      </statement>
    </definition>
    <p>
      So, before we proceed to find the recurrence relations and the
      series coefficients, we use this equation to determine <m>r</m>.
      After finding <m>r</m>, the method looks very similar to
      solutions at ordinary points. If there are two real roots of
      the indicial equation, is seems we'll have to repeat the same
      process for each root. However, in practice, we can leave
      <m>r</m> undetermined for most of the procees.
    </p>
  </subsection>
  <subsection xml:id="subsection-frobenius-examples">
    <title>Examples</title>
    <example>
      <statement>
        <p>
          <md>
            <mrow>
              3ty^{\prime \prime} + y^\prime - y \amp = 0
            </mrow>
            <mrow>
              P \amp = \frac{1}{3t}
            </mrow>
            <mrow>
              Q \amp = \frac{-1}{3t}
            </mrow>
            <mrow>
              tP \amp = \frac{1}{3} \implies p_{-1} = \frac{1}{3}
            </mrow>
            <mrow>
              t^2 Q \amp = \frac{-t}{3} \implies q_{-2} = 0
            </mrow>
            <mrow>
              r(r-1) + \frac{r}{3} \amp = 0
            </mrow>
            <mrow>
              3r^2 - 2r = 0 \implies r \amp = 0 \text{ or }  r =
              \frac{2}{3}
            </mrow>
          </md>
          We'll deal with the <m>r=0</m> case first. (Note that
          <m>r=0</m> means we get a conventional Taylor series
          solution.)
          <md>
            <mrow>
              3t \sum_{n=2}^\infty c_n (n) (n-1) t^{n-2} +
              \sum_{n=1}^\infty c_n (n) t^{n-1} - \sum_{n=0}^\infty
              c_n t^{n} \amp = 0
            </mrow>
            <mrow>
              \sum_{n=2}^\infty 3c_n (n) (n-1) t^{n-1} +
              \sum_{n=1}^\infty c_n (n) t^{n-1} - \sum_{n=0}^\infty
              c_n t^{n} \amp = 0
            </mrow>
            <mrow>
              \sum_{n=1}^\infty 3c_{n+1} (n+1) n t^n +
              \sum_{n=1}^\infty c_{n+1} (n+1) t^n - \sum_{n=0}^\infty
              c_n t^{n} \amp = 0
            </mrow>
            <mrow>
              c_1 - c_0 + \sum_{n=1}^\infty \left[ 3(n+1)n c_{n+1} +
              (n+1) c_{n+1} - c_n \right] t^n \amp = 0
            </mrow>
            <mrow>
              c_{n+1} = \frac{c_n}{3(n^2+n) + n+1} = \frac{c_n}{3n^2
              +4n +1} \amp = \frac{c_n}{(3n+1)(n+1)}
            </mrow>
          </md>
          We have the recurrence relation (which is first order this
          time), so we start calculating terms.
          <md>
            <mrow>
              c_0 \amp = c_0
            </mrow>
            <mrow>
              c_1 \amp = c_0
            </mrow>
            <mrow>
              c_2 \amp = \frac{c_1}{(4)(2)} = \frac{c_0}{(4)(2)}
            </mrow>
            <mrow>
              c_3 \amp = \frac{c_2}{(7)(3)} =
              \frac{c_0}{(7)(4)(3)(2)}
            </mrow>
            <mrow>
              c_4 \amp = \frac{c_3}{(10)(4)} =
              \frac{c_0}{(10)(7)(4)(4)(3)(2)}
            </mrow>
            <mrow>
              c_5 \amp = \frac{c_4}{(13)(5)} =
              \frac{c_0}{(13)(10)(7)(4)(5)(4)(3)(2)}
            </mrow>
            <mrow>
              c_n \amp = \frac{c_0}{n! (4)(7)(10) \ldots (3n-2)}
            </mrow>
            <mrow>
              y_1 \amp = 1 + \sum_{n=1}^\infty \frac{t^n}{n!
              (4)(7)(10) \ldots (3n-2)}
            </mrow>
          </md>
          This is the solution for <m>r=0</m>. Now we proceed to the
          <m>r=\frac{2}{3}</m> case. (This is a non-integer exponent,
          so the solution is not a conventional Taylor series.)
          <md>
            <mrow>
              3t \sum_{n=0}^\infty c_n \left(n + \frac{2}{3} \right)
              \left(n + \frac{2}{3}-1 \right) t^{n + \frac{2}{3}-2} +
              \sum_{n=0}^\infty c_n \left(n + \frac{2}{3} \right) t^{n
              + \frac{2}{3}-1} - \sum_{n=0}^\infty c_n t^{n +
              \frac{2}{3}} \amp = 0
            </mrow>
            <mrow>
              t^{\frac{2}{3}} \left[ \sum_{n=0}^\infty 3c_n \left(n+
              \frac{2}{3} \right) \left(n - \frac{1}{3} \right)
              t^{n-1} + \sum_{n=0}^\infty c_n \left(n+ \frac{2}{3}
              \right) t^{n-1} - \sum_{n=0}^\infty c_n t^{n} \right]
              \amp = 0
            </mrow>
            <mrow>
              t^{\frac{2}{3}} \left[ \sum_{n=0}^\infty 3c_n \left(n +
              \frac{2}{3} \right) \left( n - \frac{1}{3} \right) t^{n
              -1} + \sum_{n=0}^\infty c_n \left(n + \frac{2}{3}
              \right) t^{n-1} - \sum_{n=1}^\infty c_{n-1} t^{n-1}
              \right] \amp = 0
            </mrow>
            <mrow>
              t^{\frac{2}{3}} \left[ \left(3\frac{2}{3} \frac{-1}{3} +
              \frac{2}{3} \right) c_0 t^{-1} + \sum_{n=1}^\infty
              \left[ 3 c_n \left( n+ \frac{2}{3} \right) \left( n -
              \frac{1}{3} \right) + c_n \left( n + \frac{2}{3} \right)
              - c_{n-1} \right] t^{n-1} \right] \amp = 0
            </mrow>
            <mrow>
              \left[ \left( n + \frac{2}{3} \right) \left( 3n - 1 + 1
              \right) \right] c_n - c_{n-1} \amp = 0
            </mrow>
            <mrow>
              c_n = \frac{c_{n-1}}{ \left( n + \frac{2}{3} \right)
              (3n)} = \frac{c_{n-1}}{(3n+2)n} \amp
            </mrow>
            <mrow>
              c_{n+1} = \frac{c_n}{(3n+5)(n+1)} \amp
            </mrow>
          </md>
          The term in front of <m>c_0</m> reduces to <m>0</m>, so
          <m>c_0</m> is free. After the first term, we have a
          recurrence relation, so we start calculating terms.
          <md>
            <mrow>
              c_0 \amp = c_0
            </mrow>
            <mrow>
              c_1 \amp = \frac{c_0}{5}
            </mrow>
            <mrow>
              c_2 \amp = \frac{c_1}{(8)(2)} = \frac{c_0}{(8)(5)(2)}
            </mrow>
            <mrow>
              c_3 \amp = \frac{c_2}{(11)(3)} =
              \frac{c_0}{(5)(8)(11)(2)(3)}
            </mrow>
            <mrow>
              c_4 \amp = \frac{c_3}{(14)(4)} =
              \frac{c_0}{(5)(8)(11)(14)(2)(3)(4)}
            </mrow>
            <mrow>
              c_5 \amp = \frac{c_4}{(17)(5)} =
              \frac{c_0}{(5)(8)(11)(14)(17)(2)(3)(4)(5)}
            </mrow>
            <mrow>
              c_n \amp = \frac{c_0}{n! (5)(8)(11) \ldots (3n+2)}
            </mrow>
            <mrow>
              y_2 \amp = 1 + \sum_{n=1}^\infty \frac{t^{n +
              \frac{2}{3}}}{n! (5)(8) \ldots (3n+2)}
            </mrow>
            <mrow>
              y \amp = A \left( 1 + \sum_{n=1}^\infty \frac{t^n}{n!
              (4)(7)(10) \ldots (3n-2)} \right) + B \left( 1 +
              \sum_{n=1}^\infty \frac{t^{n + \frac{2}{3}}}{n! (5)(8)
              \ldots (3n+2)} \right)
            </mrow>
          </md>
          The radius of convergence here is <m>R = \infty</m>, which
          we can find by ratio test. It is good to note, though, that
          there are no guarantees about the radius of convergence with
          the method of Frobenius.
        </p>
        <p>
          In this example, we could have gone as far as the recurrence
          relation using an arbitrary <m>r</m> to avoid repetition.
          We do need to specify <m>r</m> once we get to the recurrence
          relation.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          <me>
            ty^{\prime \prime} + y = 0
          </me>
          Here <m>P =0</m> and <m>Q=\frac{1}{t}</m>, so <m>p_{-1} =
          0</m> and <m>q_{-2} = 0</m>. That means the indicial
          equation is <m>r(r-1) = 0</m> with roots <m>r=0</m> and
          <m>r=1</m>. We start with the case <m>r=0</m>.
          <md>
            <mrow>
              t \sum_{n=2}^\infty n(n-1)c_n t^{n-2} +
              \sum_{n=0}^\infty c_n t^n \amp = 0
            </mrow>
            <mrow>
              \sum_{n=2}^\infty n(n-1)c_n t^{n-1} + \sum_{n=0}^\infty
              c_n t^n \amp = 0
            </mrow>
            <mrow>
              \sum_{n=1}^\infty (n+1)nc_n t^n + \sum_{n=0}^\infty c_n
              t^n \amp = 0
            </mrow>
            <mrow>
              c_0 + \sum_{n=1}^\infty \left[ (n+1)nc_n + c_n \right]
              t^n \amp = 0
            </mrow>
            <mrow>
              c_{n+1} \amp = \frac{-c_n}{(n+1)(n)}
            </mrow>
            <mrow>
              c_0 \amp = 0
            </mrow>
            <mrow>
              c_1 \amp = c_1
            </mrow>
            <mrow>
              c_2 \amp = \frac{-c_1}{2}
            </mrow>
            <mrow>
              c_3 \amp = \frac{-c_2}{(3)(2)} = \frac{c_1}{(3)(2)}
            </mrow>
            <mrow>
              c_4 \amp = \frac{-c_3}{(4)(3)} =
              \frac{-c_1}{(4)(3)(3)(2)(2)}
            </mrow>
            <mrow>
              c_5 \amp = \frac{-c_4}{(5)(4)} =
              \frac{c_1}{(5)(4)(4)(3)(3)(2)(2)}
            </mrow>
            <mrow>
              c_n \amp = \frac{(-1)^{n+1} c_1}{n! (n-1)!}
            </mrow>
            <mrow>
              y_1 \amp = \sum_{n=1}^\infty \frac{(-1)^{n+1}
              t^n}{n!(n-1)!}
            </mrow>
          </md>
          Then we calculate with <m>r=1</m>. Note the bounds in this
          case: since the series doesn't have a constant term, we only
          loose one term in the second derivative.
          <md>
            <mrow>
              t \sum_{n=1}^\infty (n+1)nc_n t^{n-1} +
              \sum_{n=0}^\infty c_n t^{n+1} \amp = 0
            </mrow>
            <mrow>
              \sum_{n=1}^\infty (n+1)nc_n t^n + \sum_{n=0}^\infty c_n
              t^{n+1} \amp = 0
            </mrow>
            <mrow>
              \sum_{n=1}^\infty (n+1)nc_n t^n + \sum_{n=1}^\infty
              c_{n-1} t^n+ \amp = 0
            </mrow>
            <mrow>
              (n+1)n c_n + c_{n-1} \amp = 0
            </mrow>
            <mrow>
              c_n \amp = \frac{-c_{n-1}}{(n+1)(n)}
            </mrow>
            <mrow>
              c_{n+1} \amp = \frac{-c_n}{(n+2)(n+1)}
            </mrow>
            <mrow>
              c_0 \amp = c_0
            </mrow>
            <mrow>
              c_1 \amp = \frac{-c_0}{2}
            </mrow>
            <mrow>
              c_2 \amp = \frac{-c_1}{(3)(2)} = \frac{c_0}{(3)(2)}
            </mrow>
            <mrow>
              c_3 \amp = \frac{-c_2}{(4)(3)} =
              \frac{-c_0}{(4)(3)(3)(2)(2)}
            </mrow>
            <mrow>
              c_4 \amp = \frac{-c_3}{(5)(4)} =
              \frac{c_0}{(5)(4)(4)(3)(3)(2)(2)}
            </mrow>
            <mrow>
              c_n \amp = \frac{(-1)^{n} c_0}{(n+1)! n!}
            </mrow>
            <mrow>
              y_1 \amp = \sum_{n=0}^\infty \frac{(-1)^n
              t^{n+1}}{(n+1)!n!}
            </mrow>
            <mrow>
              y_1 \amp = \sum_{n=1}^\infty \frac{(-1)^{n+1}
              t^n}{n!(n-1)!}
            </mrow>
          </md>
          Very curiously, we get the same series. The two roots don't
          lead to two independent series, but to the same series. We
          would need other information to get the second solutions.
          In general, finding another solution can be quite difficult.
          With the method of Frobenius, we are not guaranteed to find
          both solutions.
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="subsection-frobenius-multiple-solutions">
    <title>Multiple Solutions in the Method of Frobenius</title>
    <p>
      We have a theorem which deals with the situation in the previous
      example, where both roots gave the same series.
    </p>
    <theorem>
      <statement>
        <p>
          In the setting of the method of Frobenius, assume that
          <m>r_1</m> and <m>r_2</m> are two real roots of the indicial
          equation. Without loss of generalization, assume that
          <m>r_1 \geq r_2</m>. Then there are three cases.
        </p>
        <p><ul>
          <li>
            <title>Case 1:</title>
            <p>
              If <m>r_1 - r_2 \notin \NN</m> then each root will
              derive a linearly independent series solutions. (The
              idea here is that different fractional or irrational
              exponents lead to radically different behaviour.)
            </p>
          </li>
          <li>
            <title>Case 2:</title>
            <p>
              If <m>r_1 - r_2 \in \NN</m> but <m>r_1 \neq r_2</m>,
              then the second root may not produce a new linearly
              independent solution. If it doesn't, and both roots
              produce the solution <m>y_1</m>, then a second solution
              exists and has the form:
              <me>
                y_2 = a y_1 \ln t + \sum_{n=0}^\infty b_n t^{n+r_2}
              </me>.
              In this case, we would need to do extra work to find the
              constant <m>a</m> and the series coefficients
              <m>b_n</m>.
            </p>
          </li>
          <li>
            <title>Case 3</title>
            <p>
              If the two roots are the same, then (obviously) only one
              series solution is generated. A second solution exists
              and has the same form as the previous case.
            </p>
          </li>
        </ul></p>
      </statement>
    </theorem>
    <p>
      The proof of this theorem relies on another differential
      equation technique called <em>reduction of order</em>. I've
      chosen not to cover that technique in this courses, but it is
      good to be aware that it exists. In general, if we have one
      solution to a second order linear DE, then reduction of order is
      a method for using that solution to change the second order DE
      into a first order DE, which can then be solved by first order
      methods.
    </p>
    <theorem>
      <title>Reduction of Order</title>
      <statement>
        <p>
          Assume that <m>y_1</m> is a solution of homogeneous linear
          second order DE of the form
          <me>
            y^{\prime \prime} + P(t) y^\prime + Q(t) = 0
          </me>.
          Then a second linearly independent is given by the formula
          <me>
            y_2 = y_1(t) \int \frac{e^{- \int P(t) dt}}{y_1^2(t)} dt
          </me>.
          This reduction of order technique is fairly impractical for
          series solution, since dividing by <m>y_1^2</m> and then
          integrating is a miserable calculation.
        </p>
      </statement>
    </theorem>
  </subsection>
</section>
